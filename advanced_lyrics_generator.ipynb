{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced RNN Lyrics Generator with RAG System\n",
        "## Generative AI Model for Ariana Grande Lyrics\n",
        "\n",
        "This notebook implements an advanced RNN neural network with:\n",
        "- **5 hidden layers** with LSTM architecture\n",
        "- **5 activation functions**: ReLU, Tanh, ELU, SELU, Sigmoid\n",
        "- **1 loss activation function**: Softmax\n",
        "- **RAG (Retrieval-Augmented Generation)** system for context-aware generation\n",
        "- **MongoDB integration** for embeddings storage\n",
        "- **TensorFlow/Keras** for deep learning\n",
        "- **PySpark** for big data processing\n",
        "- **scikit-learn** for ML utilities\n",
        "- **NumPy, pandas** for data manipulation\n",
        "- **pickle** for model persistence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Ariana Grande lyrics dataset\n",
        "df = pd.read_csv('ArianaGrande.csv')\n",
        "\n",
        "print(f\"üìä Dataset shape: {df.shape}\")\n",
        "print(f\"üìã Columns: {df.columns.tolist()}\")\n",
        "print(\"\\nüîç First few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nüìà Dataset Statistics:\")\n",
        "print(f\"Total songs: {df['Title'].nunique()}\")\n",
        "print(f\"Total albums: {df['Album'].nunique()}\")\n",
        "print(f\"Year range: {df['Year'].min()} - {df['Year'].max()}\")\n",
        "print(f\"Average lyrics length: {df['Lyric'].str.len().mean():.0f} characters\")\n",
        "\n",
        "# Display sample lyrics\n",
        "print(\"\\nüéµ Sample Lyrics:\")\n",
        "for i, lyric in enumerate(df['Lyric'].head(3)):\n",
        "    print(f\"{i+1}. {lyric[:100]}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LyricsPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = None\n",
        "        self.vocab_size = 0\n",
        "        self.max_length = 0\n",
        "        \n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and normalize lyrics text\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        \n",
        "        # Convert to lowercase\n",
        "        text = str(text).lower()\n",
        "        \n",
        "        # Remove special characters but keep basic punctuation\n",
        "        text = re.sub(r'[^\\w\\s\\.,!?;:\\'\"-]', '', text)\n",
        "        \n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        \n",
        "        # Add start and end tokens\n",
        "        text = f\"<start> {text} <end>\"\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def prepare_data(self, lyrics_list, max_vocab_size=15000, max_length=150):\n",
        "        \"\"\"Prepare data for training\"\"\"\n",
        "        print(\"üßπ Cleaning lyrics data...\")\n",
        "        \n",
        "        # Clean all lyrics\n",
        "        cleaned_lyrics = [self.clean_text(lyric) for lyric in lyrics_list]\n",
        "        \n",
        "        # Initialize tokenizer\n",
        "        self.tokenizer = Tokenizer(\n",
        "            num_words=max_vocab_size,\n",
        "            filters='',\n",
        "            oov_token='<oov>'\n",
        "        )\n",
        "        \n",
        "        # Fit tokenizer on cleaned lyrics\n",
        "        self.tokenizer.fit_on_texts(cleaned_lyrics)\n",
        "        \n",
        "        # Convert texts to sequences\n",
        "        sequences = self.tokenizer.texts_to_sequences(cleaned_lyrics)\n",
        "        \n",
        "        # Pad sequences\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "        \n",
        "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        print(f\"‚úÖ Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"‚úÖ Max sequence length: {self.max_length}\")\n",
        "        print(f\"‚úÖ Total sequences: {len(sequences)}\")\n",
        "        \n",
        "        return padded_sequences, cleaned_lyrics\n",
        "    \n",
        "    def create_training_data(self, sequences):\n",
        "        \"\"\"Create input-output pairs for training\"\"\"\n",
        "        print(\"üîÑ Creating training sequences...\")\n",
        "        X, y = [], []\n",
        "        \n",
        "        for sequence in sequences:\n",
        "            for i in range(1, len(sequence)):\n",
        "                if sequence[i] != 0:  # Skip padding tokens\n",
        "                    X.append(sequence[:i])\n",
        "                    y.append(sequence[i])\n",
        "        \n",
        "        # Pad input sequences\n",
        "        X = pad_sequences(X, maxlen=self.max_length-1, padding='post')\n",
        "        \n",
        "        print(f\"‚úÖ Training samples: {len(X)}\")\n",
        "        print(f\"‚úÖ Input shape: {X.shape}\")\n",
        "        print(f\"‚úÖ Output shape: {y.shape}\")\n",
        "        \n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = LyricsPreprocessor()\n",
        "\n",
        "# Prepare data\n",
        "sequences, cleaned_lyrics = preprocessor.prepare_data(\n",
        "    df['Lyric'].tolist(),\n",
        "    max_vocab_size=15000,\n",
        "    max_length=150\n",
        ")\n",
        "\n",
        "# Create training data\n",
        "X, y = preprocessor.create_training_data(sequences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Advanced RNN Architecture with 5 Hidden Layers and 5 Activation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedRNNGenerator:\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_units=512, max_length=150):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_units = hidden_units\n",
        "        self.max_length = max_length\n",
        "        self.model = None\n",
        "        \n",
        "    def build_model(self):\n",
        "        \"\"\"Build advanced RNN model with 5 hidden layers and multiple activation functions\"\"\"\n",
        "        print(\"üèóÔ∏è Building Advanced RNN Architecture...\")\n",
        "        \n",
        "        model = models.Sequential()\n",
        "        \n",
        "        # Input layer - Embedding\n",
        "        model.add(layers.Embedding(\n",
        "            input_dim=self.vocab_size,\n",
        "            output_dim=self.embedding_dim,\n",
        "            input_length=self.max_length-1,\n",
        "            name='embedding_layer'\n",
        "        ))\n",
        "        \n",
        "        # Hidden Layer 1: LSTM with ReLU activation\n",
        "        model.add(layers.LSTM(\n",
        "            units=self.hidden_units,\n",
        "            return_sequences=True,\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2,\n",
        "            name='lstm_layer_1'\n",
        "        ))\n",
        "        model.add(layers.Activation('relu', name='activation_relu'))\n",
        "        \n",
        "        # Hidden Layer 2: LSTM with Tanh activation\n",
        "        model.add(layers.LSTM(\n",
        "            units=self.hidden_units,\n",
        "            return_sequences=True,\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2,\n",
        "            name='lstm_layer_2'\n",
        "        ))\n",
        "        model.add(layers.Activation('tanh', name='activation_tanh'))\n",
        "        \n",
        "        # Hidden Layer 3: LSTM with ELU activation\n",
        "        model.add(layers.LSTM(\n",
        "            units=self.hidden_units,\n",
        "            return_sequences=True,\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2,\n",
        "            name='lstm_layer_3'\n",
        "        ))\n",
        "        model.add(layers.Activation('elu', name='activation_elu'))\n",
        "        \n",
        "        # Hidden Layer 4: LSTM with SELU activation\n",
        "        model.add(layers.LSTM(\n",
        "            units=self.hidden_units,\n",
        "            return_sequences=True,\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2,\n",
        "            name='lstm_layer_4'\n",
        "        ))\n",
        "        model.add(layers.Activation('selu', name='activation_selu'))\n",
        "        \n",
        "        # Hidden Layer 5: LSTM with Sigmoid activation\n",
        "        model.add(layers.LSTM(\n",
        "            units=self.hidden_units,\n",
        "            return_sequences=False,\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2,\n",
        "            name='lstm_layer_5'\n",
        "        ))\n",
        "        model.add(layers.Activation('sigmoid', name='activation_sigmoid'))\n",
        "        \n",
        "        # Dense layer for output\n",
        "        model.add(layers.Dense(\n",
        "            units=self.hidden_units,\n",
        "            activation='relu',\n",
        "            name='dense_layer_1'\n",
        "        ))\n",
        "        \n",
        "        # Output layer with softmax activation (loss function)\n",
        "        model.add(layers.Dense(\n",
        "            units=self.vocab_size,\n",
        "            activation='softmax',\n",
        "            name='output_layer'\n",
        "        ))\n",
        "        \n",
        "        # Compile model with advanced optimizer\n",
        "        optimizer = optimizers.Adam(\n",
        "            learning_rate=0.001,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            epsilon=1e-07\n",
        "        )\n",
        "        \n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        self.model = model\n",
        "        print(\"‚úÖ Advanced RNN Model Built Successfully!\")\n",
        "        return model\n",
        "    \n",
        "    def get_model_summary(self):\n",
        "        \"\"\"Get model architecture summary\"\"\"\n",
        "        if self.model:\n",
        "            return self.model.summary()\n",
        "        return None\n",
        "\n",
        "# Initialize the advanced RNN generator\n",
        "rnn_generator = AdvancedRNNGenerator(\n",
        "    vocab_size=preprocessor.vocab_size,\n",
        "    embedding_dim=256,\n",
        "    hidden_units=512,\n",
        "    max_length=preprocessor.max_length\n",
        ")\n",
        "\n",
        "# Build the model\n",
        "model = rnn_generator.build_model()\n",
        "\n",
        "print(\"\\nüìã Model Architecture Summary:\")\n",
        "print(rnn_generator.get_model_summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
